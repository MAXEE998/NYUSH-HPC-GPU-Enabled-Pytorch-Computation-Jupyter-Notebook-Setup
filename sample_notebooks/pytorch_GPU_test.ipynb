{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import torch, check torch version, and CUDA availablity\n",
    "\n",
    "## *Caveats:*\n",
    "1. loading a module typically takes minutes\n",
    "2. The GPU node does not have network connection, so either download your data on the access node or upload the datd before using the GPU node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8.1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a tensor and send it to the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.]]], device='cuda:0')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.ones(100,100,100)\n",
    "t.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a transformer-based NLP Model with the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "        self.model_type = 'Transformer'\n",
    "        self.pos_encoder = PositionalEncoding(ninp, dropout)\n",
    "        encoder_layers = TransformerEncoderLayer(ninp, nhead, nhid, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.encoder = nn.Embedding(ntoken, ninp)\n",
    "        self.ninp = ninp\n",
    "        self.decoder = nn.Linear(ninp, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src, src_mask):\n",
    "        src = self.encoder(src) * math.sqrt(self.ninp)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src, src_mask)\n",
    "        output = self.decoder(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.datasets import WikiText2\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from collections import Counter\n",
    "from torchtext.vocab import Vocab\n",
    "\n",
    "train_iter = WikiText2(split='train')\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "counter = Counter()\n",
    "for line in train_iter:\n",
    "    counter.update(tokenizer(line))\n",
    "vocab = Vocab(counter)\n",
    "\n",
    "def data_process(raw_text_iter):\n",
    "  data = [torch.tensor([vocab[token] for token in tokenizer(item)],\n",
    "                       dtype=torch.long) for item in raw_text_iter]\n",
    "  return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))\n",
    "\n",
    "train_iter, val_iter, test_iter = WikiText2()\n",
    "train_data = data_process(train_iter)\n",
    "val_data = data_process(val_iter)\n",
    "test_data = data_process(test_iter)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def batchify(data, bsz):\n",
    "    # Divide the dataset into bsz parts.\n",
    "    nbatch = data.size(0) // bsz\n",
    "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
    "    data = data.narrow(0, 0, nbatch * bsz)\n",
    "    # Evenly divide the data across the bsz batches.\n",
    "    data = data.view(bsz, -1).t().contiguous()\n",
    "    return data.to(device)\n",
    "\n",
    "batch_size = 20\n",
    "eval_batch_size = 10\n",
    "train_data = batchify(train_data, batch_size)\n",
    "val_data = batchify(val_data, eval_batch_size)\n",
    "test_data = batchify(test_data, eval_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bptt = 35\n",
    "def get_batch(source, i):\n",
    "    seq_len = min(bptt, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len].reshape(-1)\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntokens = len(vocab.stoi) # the size of vocabulary\n",
    "emsize = 200 # embedding dimension\n",
    "nhid = 200 # the dimension of the feedforward network model in nn.TransformerEncoder\n",
    "nlayers = 2 # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "nhead = 2 # the number of heads in the multiheadattention models\n",
    "dropout = 0.2 # the dropout value\n",
    "model = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 5.0 # learning rate\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
    "\n",
    "import time\n",
    "def train():\n",
    "    model.train() # Turn on the train mode\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "    src_mask = model.generate_square_subsequent_mask(bptt).to(device)\n",
    "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
    "        data, targets = get_batch(train_data, i)\n",
    "        optimizer.zero_grad()\n",
    "        if data.size(0) != bptt:\n",
    "            src_mask = model.generate_square_subsequent_mask(data.size(0)).to(device)\n",
    "        output = model(data, src_mask)\n",
    "        loss = criterion(output.view(-1, ntokens), targets)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        log_interval = 200\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            cur_loss = total_loss / log_interval\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | '\n",
    "                  'lr {:02.2f} | ms/batch {:5.2f} | '\n",
    "                  'loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                    epoch, batch, len(train_data) // bptt, scheduler.get_lr()[0],\n",
    "                    elapsed * 1000 / log_interval,\n",
    "                    cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "def evaluate(eval_model, data_source):\n",
    "    eval_model.eval() # Turn on the evaluation mode\n",
    "    total_loss = 0.\n",
    "    src_mask = model.generate_square_subsequent_mask(bptt).to(device)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, data_source.size(0) - 1, bptt):\n",
    "            data, targets = get_batch(data_source, i)\n",
    "            if data.size(0) != bptt:\n",
    "                src_mask = model.generate_square_subsequent_mask(data.size(0)).to(device)\n",
    "            output = eval_model(data, src_mask)\n",
    "            output_flat = output.view(-1, ntokens)\n",
    "            total_loss += len(data) * criterion(output_flat, targets).item()\n",
    "    return total_loss / (len(data_source) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfsnyu/home/hl2752/hongyi/torch_env/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:369: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   200/ 2928 batches | lr 5.00 | ms/batch 38.55 | loss  8.09 | ppl  3259.90\n",
      "| epoch   1 |   400/ 2928 batches | lr 5.00 | ms/batch 11.04 | loss  6.87 | ppl   964.48\n",
      "| epoch   1 |   600/ 2928 batches | lr 5.00 | ms/batch 11.04 | loss  6.42 | ppl   615.92\n",
      "| epoch   1 |   800/ 2928 batches | lr 5.00 | ms/batch 11.04 | loss  6.29 | ppl   539.08\n",
      "| epoch   1 |  1000/ 2928 batches | lr 5.00 | ms/batch 11.03 | loss  6.17 | ppl   480.14\n",
      "| epoch   1 |  1200/ 2928 batches | lr 5.00 | ms/batch 11.04 | loss  6.15 | ppl   468.27\n",
      "| epoch   1 |  1400/ 2928 batches | lr 5.00 | ms/batch 11.05 | loss  6.11 | ppl   451.30\n",
      "| epoch   1 |  1600/ 2928 batches | lr 5.00 | ms/batch 11.07 | loss  6.10 | ppl   444.83\n",
      "| epoch   1 |  1800/ 2928 batches | lr 5.00 | ms/batch 11.01 | loss  6.01 | ppl   409.10\n",
      "| epoch   1 |  2000/ 2928 batches | lr 5.00 | ms/batch 11.02 | loss  6.01 | ppl   407.19\n",
      "| epoch   1 |  2200/ 2928 batches | lr 5.00 | ms/batch 11.05 | loss  5.89 | ppl   361.50\n",
      "| epoch   1 |  2400/ 2928 batches | lr 5.00 | ms/batch 11.07 | loss  5.96 | ppl   389.31\n",
      "| epoch   1 |  2600/ 2928 batches | lr 5.00 | ms/batch 11.14 | loss  5.95 | ppl   382.68\n",
      "| epoch   1 |  2800/ 2928 batches | lr 5.00 | ms/batch 11.08 | loss  5.88 | ppl   356.38\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 39.51s | valid loss  5.78 | valid ppl   324.09\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |   200/ 2928 batches | lr 4.51 | ms/batch 11.13 | loss  5.86 | ppl   350.26\n",
      "| epoch   2 |   400/ 2928 batches | lr 4.51 | ms/batch 11.10 | loss  5.84 | ppl   344.89\n",
      "| epoch   2 |   600/ 2928 batches | lr 4.51 | ms/batch 11.09 | loss  5.66 | ppl   286.67\n",
      "| epoch   2 |   800/ 2928 batches | lr 4.51 | ms/batch 11.08 | loss  5.69 | ppl   297.07\n",
      "| epoch   2 |  1000/ 2928 batches | lr 4.51 | ms/batch 11.10 | loss  5.65 | ppl   283.37\n",
      "| epoch   2 |  1200/ 2928 batches | lr 4.51 | ms/batch 11.09 | loss  5.68 | ppl   292.28\n",
      "| epoch   2 |  1400/ 2928 batches | lr 4.51 | ms/batch 11.11 | loss  5.68 | ppl   294.06\n",
      "| epoch   2 |  1600/ 2928 batches | lr 4.51 | ms/batch 11.12 | loss  5.71 | ppl   300.85\n",
      "| epoch   2 |  1800/ 2928 batches | lr 4.51 | ms/batch 11.12 | loss  5.64 | ppl   282.87\n",
      "| epoch   2 |  2000/ 2928 batches | lr 4.51 | ms/batch 11.12 | loss  5.66 | ppl   287.14\n",
      "| epoch   2 |  2200/ 2928 batches | lr 4.51 | ms/batch 11.16 | loss  5.54 | ppl   254.84\n",
      "| epoch   2 |  2400/ 2928 batches | lr 4.51 | ms/batch 11.13 | loss  5.64 | ppl   280.06\n",
      "| epoch   2 |  2600/ 2928 batches | lr 4.51 | ms/batch 11.12 | loss  5.64 | ppl   280.94\n",
      "| epoch   2 |  2800/ 2928 batches | lr 4.51 | ms/batch 11.14 | loss  5.57 | ppl   262.79\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 34.15s | valid loss  5.69 | valid ppl   294.72\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |   200/ 2928 batches | lr 4.29 | ms/batch 11.18 | loss  5.59 | ppl   267.63\n",
      "| epoch   3 |   400/ 2928 batches | lr 4.29 | ms/batch 11.13 | loss  5.59 | ppl   269.06\n",
      "| epoch   3 |   600/ 2928 batches | lr 4.29 | ms/batch 11.15 | loss  5.40 | ppl   221.79\n",
      "| epoch   3 |   800/ 2928 batches | lr 4.29 | ms/batch 11.15 | loss  5.47 | ppl   237.23\n",
      "| epoch   3 |  1000/ 2928 batches | lr 4.29 | ms/batch 11.19 | loss  5.42 | ppl   225.14\n",
      "| epoch   3 |  1200/ 2928 batches | lr 4.29 | ms/batch 11.26 | loss  5.45 | ppl   233.38\n",
      "| epoch   3 |  1400/ 2928 batches | lr 4.29 | ms/batch 11.33 | loss  5.48 | ppl   238.74\n",
      "| epoch   3 |  1600/ 2928 batches | lr 4.29 | ms/batch 11.33 | loss  5.50 | ppl   244.48\n",
      "| epoch   3 |  1800/ 2928 batches | lr 4.29 | ms/batch 11.34 | loss  5.44 | ppl   230.30\n",
      "| epoch   3 |  2000/ 2928 batches | lr 4.29 | ms/batch 11.41 | loss  5.47 | ppl   236.64\n",
      "| epoch   3 |  2200/ 2928 batches | lr 4.29 | ms/batch 11.35 | loss  5.34 | ppl   208.02\n",
      "| epoch   3 |  2400/ 2928 batches | lr 4.29 | ms/batch 11.43 | loss  5.44 | ppl   230.23\n",
      "| epoch   3 |  2600/ 2928 batches | lr 4.29 | ms/batch 11.40 | loss  5.45 | ppl   232.80\n",
      "| epoch   3 |  2800/ 2928 batches | lr 4.29 | ms/batch 11.39 | loss  5.38 | ppl   218.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 34.69s | valid loss  5.60 | valid ppl   271.10\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = float(\"inf\")\n",
    "epochs = 3 # The number of epochs\n",
    "best_model = None\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train()\n",
    "    val_loss = evaluate(model, val_data)\n",
    "    print('-' * 89)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
    "          'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
    "                                     val_loss, math.exp(val_loss)))\n",
    "    print('-' * 89)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model = model\n",
    "\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================================================\n",
      "| End of training | test loss  5.51 | test ppl   247.71\n",
      "=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "test_loss = evaluate(best_model, test_data)\n",
    "print('=' * 89)\n",
    "print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n",
    "    test_loss, math.exp(test_loss)))\n",
    "print('=' * 89)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
